{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset From CSV and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load Verilog and CSV data, create local embeddings, \n",
    "    and save a specific FAISS index.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Environment Variables ---\n",
    "    load_dotenv()\n",
    "\n",
    "    # --- 2. Define Paths ---\n",
    "    DATASET_PATH = \"../data/verilog_datasets\"\n",
    "    INDEX_PATH = os.path.join(DATASET_PATH, \"faiss_qft_verieval\")\n",
    "    QFT_FOLDER_PATH = os.path.join(DATASET_PATH, \"qft_pipelined\")\n",
    "    VERILOGEVAL_CSV_PATH = os.path.join(DATASET_PATH, \"verilogeval-v2.csv\")\n",
    "\n",
    "\n",
    "    # --- 3. Clean up old database directory ---\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        print(f\"--- Deleting old FAISS index directory: '{INDEX_PATH}' ---\")\n",
    "        try:\n",
    "            shutil.rmtree(INDEX_PATH)\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting directory {INDEX_PATH}: {e.strerror}\")\n",
    "            print(\"Please ensure no other programs are using this directory.\")\n",
    "            return\n",
    "    print(\"--- Starting with a fresh index directory. ---\")\n",
    "\n",
    "    # --- 4. Load Documents ---\n",
    "    all_docs = []\n",
    "    \n",
    "    # Load Verilog and Verilog Header files from the qft_pipelined directory\n",
    "    print(f\"\\n--- Loading Verilog documents from '{QFT_FOLDER_PATH}' ---\")\n",
    "    if not os.path.exists(QFT_FOLDER_PATH):\n",
    "        print(f\"ERROR: QFT directory not found at '{QFT_FOLDER_PATH}'\")\n",
    "    else:\n",
    "        # Load Verilog files as plain text, as the LanguageParser does not support Verilog\n",
    "        loader_verilog = DirectoryLoader(\n",
    "            QFT_FOLDER_PATH,\n",
    "            glob=[\"**/*.v\", \"**/*.vh\"], # UPDATED: Include .vh files\n",
    "            loader_cls=TextLoader\n",
    "        )\n",
    "        verilog_docs = loader_verilog.load()\n",
    "        all_docs.extend(verilog_docs)\n",
    "        print(f\"Successfully loaded {len(verilog_docs)} Verilog (.v & .vh) files.\")\n",
    "\n",
    "    # Load the specific CSV file\n",
    "    print(f\"\\n--- Loading documents from '{VERILOGEVAL_CSV_PATH}' ---\")\n",
    "    if not os.path.exists(VERILOGEVAL_CSV_PATH):\n",
    "        print(f\"ERROR: CSV file not found at '{VERILOGEVAL_CSV_PATH}'\")\n",
    "    else:\n",
    "        loader_csv = CSVLoader(\n",
    "            file_path=VERILOGEVAL_CSV_PATH,\n",
    "            source_column=\"instruction\",\n",
    "            csv_args={'delimiter': ',', 'quotechar': '\"'}\n",
    "        )\n",
    "        try:\n",
    "            csv_docs = loader_csv.load()\n",
    "            all_docs.extend(csv_docs)\n",
    "            print(f\"Successfully loaded content from {len(csv_docs)} rows in the CSV.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - ERROR loading file {VERILOGEVAL_CSV_PATH}: {e}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"\\nNo documents were loaded. Exiting.\")\n",
    "        return\n",
    "    print(f\"\\nTotal documents loaded: {len(all_docs)}.\")\n",
    "\n",
    "    # --- 5. Split Documents into Chunks ---\n",
    "    print(\"\\n--- Splitting documents into smaller chunks ---\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    chunked_docs = text_splitter.split_documents(all_docs)\n",
    "    print(f\"Split the documents into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "    # --- 6. Create Embeddings and Persist to FAISS ---\n",
    "    print(\"\\n--- Creating local embeddings and building FAISS index ---\")\n",
    "    \n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='all-MiniLM-L6-v2',\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "        )\n",
    "        print(f\"Embedding model loaded onto {'GPU' if torch.cuda.is_available() else 'CPU'}.\")\n",
    "\n",
    "        # --- ROBUST, TWO-STAGE INITIALIZATION FOR FAISS ---\n",
    "        batch_size = 1024\n",
    "        if not chunked_docs:\n",
    "            print(\"No chunks to process. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # Stage 1: Initialize the vector store with the first batch.\n",
    "        print(\"Initializing FAISS index with the first batch...\")\n",
    "        first_batch = chunked_docs[:batch_size]\n",
    "        vectorstore = FAISS.from_documents(documents=first_batch, embedding=embeddings)\n",
    "\n",
    "        # Stage 2: Add the rest in batches with a progress bar.\n",
    "        remaining_chunks = chunked_docs[batch_size:]\n",
    "        num_batches = math.ceil(len(remaining_chunks) / batch_size)\n",
    "        for i in tqdm(range(num_batches), desc=\"Embedding and Storing in FAISS\"):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch = remaining_chunks[start:end]\n",
    "            if batch:\n",
    "                vectorstore.add_documents(batch)\n",
    "            \n",
    "        print(\"\\nSuccessfully created and populated FAISS index.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN UNEXPECTED ERROR OCCURRED: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 7. Save Final Index ---\n",
    "    print(\"\\n--- Saving final FAISS index to disk ---\")\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    print(f\"FAISS index is stored in the folder: '{os.path.abspath(INDEX_PATH)}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset From Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to crawl the entire Lcapy documentation and create a FAISS index.\n",
    "    \"\"\"\n",
    "    # --- 1. Define Configuration ---\n",
    "    START_URL = \"https://lcapy.readthedocs.io/en/latest/\"\n",
    "    TOC_LINK_SELECTOR = \".toctree-wrapper a\"\n",
    "    INDEX_PATH = \"faiss_index_lcapy_docs\"\n",
    "\n",
    "    # --- 2. Clean up old index directory ---\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        print(f\"--- Deleting old FAISS index directory: '{INDEX_PATH}' ---\")\n",
    "        shutil.rmtree(INDEX_PATH)\n",
    "    print(\"--- Starting with a fresh index directory. ---\")\n",
    "\n",
    "    # --- 3. Discover all URLs using Requests and BeautifulSoup ---\n",
    "    print(f\"\\n--- Step 1: Discovering all documentation URLs from '{START_URL}' ---\")\n",
    "    urls_to_crawl = [START_URL] # Always include the main page\n",
    "    try:\n",
    "        response = requests.get(START_URL)\n",
    "        response.raise_for_status() # Will raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all link elements (<a>) that match the CSS selector\n",
    "        links = soup.select(TOC_LINK_SELECTOR)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                # Convert relative links (e.g., 'netlists.html') to full URLs\n",
    "                full_url = urljoin(START_URL, href)\n",
    "                if full_url not in urls_to_crawl:\n",
    "                    urls_to_crawl.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(urls_to_crawl)} total pages to crawl.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED DURING URL DISCOVERY: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Crawl each discovered URL using crawl4ai ---\n",
    "    print(\"\\n--- Step 2: Crawling each page individually ---\")\n",
    "    all_docs = []\n",
    "    try:\n",
    "        async with AsyncWebCrawler() as crawler:\n",
    "            # Use tqdm to show a progress bar for the crawling step\n",
    "            for url in tqdm(urls_to_crawl, desc=\"Crawling Pages\"):\n",
    "                result = await crawler.arun(url=url)\n",
    "                if result and result.markdown:\n",
    "                    doc = Document(page_content=result.markdown, metadata={\"source\": result.url})\n",
    "                    all_docs.append(doc)\n",
    "    \n",
    "        if not all_docs:\n",
    "            print(\"\\nCrawling did not return any documents. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nSuccessfully crawled {len(all_docs)} pages.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED DURING CRAWLING: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 5. Split Documents into Chunks ---\n",
    "    print(\"\\n--- Splitting documents into smaller chunks ---\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    chunked_docs = text_splitter.split_documents(all_docs)\n",
    "    print(f\"Split the documents into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "    # --- 6. Create Embeddings and Persist to FAISS ---\n",
    "    print(\"\\n--- Creating local embeddings and building FAISS index ---\")\n",
    "    \n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='all-MiniLM-L6-v2',\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "        )\n",
    "        print(f\"Embedding model loaded onto {'GPU' if torch.cuda.is_available() else 'CPU'}.\")\n",
    "\n",
    "        batch_size = 512\n",
    "        if not chunked_docs:\n",
    "            print(\"No chunks to process. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(\"Initializing FAISS index with the first batch...\")\n",
    "        first_batch = chunked_docs[:batch_size]\n",
    "        vectorstore = FAISS.from_documents(documents=first_batch, embedding=embeddings)\n",
    "\n",
    "        remaining_chunks = chunked_docs[batch_size:]\n",
    "        num_batches = math.ceil(len(remaining_chunks) / batch_size)\n",
    "        for i in tqdm(range(num_batches), desc=\"Embedding and Storing in FAISS\"):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch = remaining_chunks[start:end]\n",
    "            if batch:\n",
    "                vectorstore.add_documents(batch)\n",
    "            \n",
    "        print(\"\\nSuccessfully created and populated FAISS index.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN UNEXPECTED ERROR OCCURRED DURING INDEXING: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 7. Save Final Index ---\n",
    "    print(\"\\n--- Saving final FAISS index to disk ---\")\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    print(f\"\\n--- RAG data saved successfully! ---\")\n",
    "    print(f\"FAISS index for Lcapy docs is stored in the folder: '{os.path.abspath(INDEX_PATH)}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function to crawl the entire PySpice documentation and create a FAISS index.\n",
    "    \"\"\"\n",
    "    # --- 1. Define Configuration ---\n",
    "    START_URL = \"https://pyspice.fabrice-salvaire.fr/releases/v1.5/index.html\"\n",
    "    TOC_LINK_SELECTOR = \".toctree-wrapper a\"\n",
    "    INDEX_PATH = \"faiss_index_pyspice_docs\"\n",
    "\n",
    "    # --- 2. Clean up old index directory ---\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        print(f\"--- Deleting old FAISS index directory: '{INDEX_PATH}' ---\")\n",
    "        shutil.rmtree(INDEX_PATH)\n",
    "    print(\"--- Starting with a fresh index directory. ---\")\n",
    "\n",
    "    # --- 3. Discover all URLs using Requests and BeautifulSoup ---\n",
    "    print(f\"\\n--- Step 1: Discovering all documentation URLs from '{START_URL}' ---\")\n",
    "    urls_to_crawl = [START_URL] # Always include the main page\n",
    "    try:\n",
    "        response = requests.get(START_URL)\n",
    "        response.raise_for_status() # Will raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all link elements (<a>) that match the CSS selector\n",
    "        links = soup.select(TOC_LINK_SELECTOR)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                # Convert relative links (e.g., 'netlists.html') to full URLs\n",
    "                full_url = urljoin(START_URL, href)\n",
    "                if full_url not in urls_to_crawl:\n",
    "                    urls_to_crawl.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(urls_to_crawl)} total pages to crawl.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED DURING URL DISCOVERY: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Crawl each discovered URL using crawl4ai ---\n",
    "    print(\"\\n--- Step 2: Crawling each page individually ---\")\n",
    "    all_docs = []\n",
    "    try:\n",
    "        async with AsyncWebCrawler() as crawler:\n",
    "            # Use tqdm to show a progress bar for the crawling step\n",
    "            for url in tqdm(urls_to_crawl, desc=\"Crawling Pages\"):\n",
    "                result = await crawler.arun(url=url)\n",
    "                if result and result.markdown:\n",
    "                    doc = Document(page_content=result.markdown, metadata={\"source\": result.url})\n",
    "                    all_docs.append(doc)\n",
    "    \n",
    "        if not all_docs:\n",
    "            print(\"\\nCrawling did not return any documents. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nSuccessfully crawled {len(all_docs)} pages.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN ERROR OCCURRED DURING CRAWLING: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 5. Split Documents into Chunks ---\n",
    "    print(\"\\n--- Splitting documents into smaller chunks ---\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    chunked_docs = text_splitter.split_documents(all_docs)\n",
    "    print(f\"Split the documents into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "    # --- 6. Create Embeddings and Persist to FAISS ---\n",
    "    print(\"\\n--- Creating local embeddings and building FAISS index ---\")\n",
    "    \n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name='all-MiniLM-L6-v2',\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "        )\n",
    "        print(f\"Embedding model loaded onto {'GPU' if torch.cuda.is_available() else 'CPU'}.\")\n",
    "\n",
    "        batch_size = 512\n",
    "        if not chunked_docs:\n",
    "            print(\"No chunks to process. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(\"Initializing FAISS index with the first batch...\")\n",
    "        first_batch = chunked_docs[:batch_size]\n",
    "        vectorstore = FAISS.from_documents(documents=first_batch, embedding=embeddings)\n",
    "\n",
    "        remaining_chunks = chunked_docs[batch_size:]\n",
    "        num_batches = math.ceil(len(remaining_chunks) / batch_size)\n",
    "        for i in tqdm(range(num_batches), desc=\"Embedding and Storing in FAISS\"):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch = remaining_chunks[start:end]\n",
    "            if batch:\n",
    "                vectorstore.add_documents(batch)\n",
    "            \n",
    "        print(\"\\nSuccessfully created and populated FAISS index.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAN UNEXPECTED ERROR OCCURRED DURING INDEXING: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 7. Save Final Index ---\n",
    "    print(\"\\n--- Saving final FAISS index to disk ---\")\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    print(f\"\\n--- RAG data saved successfully! ---\")\n",
    "    print(f\"FAISS index for PySpice docs is stored in the folder: '{os.path.abspath(INDEX_PATH)}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl From Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from googlesearch import search\n",
    "import nest_asyncio\n",
    "from github import Github, UnknownObjectException\n",
    "import re\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_repo_name_from_url(url):\n",
    "    \"\"\"Extracts 'username/repository' from a GitHub URL.\"\"\"\n",
    "    match = re.search(r\"github\\.com/([^/]+/[^/]+)\", url)\n",
    "    if match:\n",
    "        repo_name = match.group(1)\n",
    "        # Remove .git suffix if present\n",
    "        if repo_name.endswith('.git'):\n",
    "            return repo_name[:-4]\n",
    "        return repo_name\n",
    "    return None\n",
    "\n",
    "def get_repo_files_recursive(repo, path=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively fetches the content of all text-based files in a repository.\n",
    "    Returns a list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    \n",
    "    # Common text/code file extensions to include\n",
    "    text_extensions = ['.v', '.sv', '.vhd', '.py', '.md', '.txt', '.c', '.h', '.cpp', '.hpp', '.js', '.html', '.css', '.json', '.xml']\n",
    "\n",
    "    try:\n",
    "        contents = repo.get_contents(path)\n",
    "        for content_file in contents:\n",
    "            if content_file.type == \"dir\":\n",
    "                # If it's a directory, recurse into it\n",
    "                print(f\"  Entering directory: {content_file.path}\")\n",
    "                all_docs.extend(get_repo_files_recursive(repo, content_file.path))\n",
    "            else:\n",
    "                # If it's a file, check its extension\n",
    "                if any(content_file.name.endswith(ext) for ext in text_extensions):\n",
    "                    print(f\"  Fetching file: {content_file.path}\")\n",
    "                    try:\n",
    "                        # Decode content and create a LangChain Document\n",
    "                        file_content = content_file.decoded_content.decode('utf-8')\n",
    "                        doc = Document(page_content=file_content, metadata={\"source\": content_file.path})\n",
    "                        all_docs.append(doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    - Could not decode file {content_file.path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get contents for path '{path}'. It might be a submodule. Error: {e}\")\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# --- Main Application Logic ---\n",
    "\n",
    "async def main():\n",
    "    query = \"risc v verilog github\"\n",
    "    print(f\"Searching for: {query}\\n\")\n",
    "\n",
    "    # --- Step 1: Search and let user choose a repository ---\n",
    "    try:\n",
    "        urls = list(search(query, num_results=5, lang=\"en\"))\n",
    "        github_urls = [url for url in urls if \"github.com\" in url]\n",
    "        if not github_urls:\n",
    "            print(\"No GitHub repositories found in the top search results.\")\n",
    "            return\n",
    "\n",
    "        print(\"Found the following GitHub repositories:\")\n",
    "        for i, url in enumerate(github_urls):\n",
    "            print(f\"  {i+1}: {url}\")\n",
    "\n",
    "        choice = -1\n",
    "        while choice < 1 or choice > len(github_urls):\n",
    "            try:\n",
    "                user_input = input(f\"\\nPlease enter the number of the repo to process (1-{len(github_urls)}): \")\n",
    "                choice = int(user_input)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "        \n",
    "        repo_name = get_repo_name_from_url(github_urls[choice - 1])\n",
    "        if not repo_name:\n",
    "            print(\"Could not extract a valid repository name.\")\n",
    "            return\n",
    "\n",
    "        # --- Step 2: Recursively fetch all file contents from the repo ---\n",
    "        print(f\"\\nInspecting repository: {repo_name}\")\n",
    "        g = Github() \n",
    "        repo = g.get_repo(repo_name)\n",
    "        \n",
    "        print(\"\\n--- Starting to retrieve all files from repository ---\")\n",
    "        documents = get_repo_files_recursive(repo)\n",
    "        \n",
    "        if not documents:\n",
    "            print(\"\\nNo text-based files found to process.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nSuccessfully retrieved {len(documents)} files.\")\n",
    "\n",
    "        # --- Step 3: Chunk the documents ---\n",
    "        print(\"\\n--- Splitting documents into smaller chunks ---\")\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        chunked_docs = text_splitter.split_documents(documents)\n",
    "        print(f\"Split the documents into {len(chunked_docs)} chunks.\")\n",
    "\n",
    "        # --- Step 4: Create embeddings and FAISS index ---\n",
    "        print(\"\\n--- Creating embeddings and building FAISS index (this may take a while) ---\")\n",
    "        # Use a popular, lightweight sentence-transformer model\n",
    "        embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "        \n",
    "        # This one command creates embeddings and the FAISS index\n",
    "        vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
    "        print(\"Successfully created FAISS index from documents.\")\n",
    "\n",
    "        # --- Step 5: Save the FAISS index to disk ---\n",
    "        save_path = f\"faiss_index_{repo_name.replace('/', '_')}\"\n",
    "        vectorstore.save_local(save_path)\n",
    "        print(f\"\\n--- RAG data saved successfully! ---\")\n",
    "        print(f\"FAISS index and documents are stored in the folder: '{os.path.abspath(save_path)}'\")\n",
    "        print(\"You can now load this index for your RAG application.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "# Run the main asynchronous function\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chipster_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
